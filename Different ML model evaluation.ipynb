{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing machine learning models in scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- How do I choose **which model to use** for my supervised learning task?\n",
    "- How do I choose the **best tuning parameters** for that model?\n",
    "- How do I estimate the **likely performance of my model** on out-of-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "- Classification task: Predicting the species of an unknown iris\n",
    "- Used three classification models: KNN (K=1), KNN (K=5), logistic regression\n",
    "- Need a way to choose between the models. Which model performed the best among them.\n",
    "\n",
    "**Solution:** Model evaluation procedures\n",
    "- **Model Evaluation** process allows us to estimates how well a given model is likely to perfom on our sample data.\n",
    "\n",
    "- There are many *Model evaluation* procedure. But we'll see only two procedure here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation procedure #1: Train and test on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the model on the **entire dataset**.\n",
    "2. Test the model on the **same dataset** by checking how well they performed on that same data, and evaluate how well we did by comparing the **predicted** response values with the **true** response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with training data\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# predict the response values for the observations in X\n",
    "logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the predicted response values\n",
    "y_pred = logreg.predict(X)\n",
    "\n",
    "# check how many predictions were generated. in output we get 150 prediction, which is 1 prediction for each \n",
    "# observation\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy: a numerical way to check how well our model performed. Most obvious choice would be classification accuracy.\n",
    "\n",
    "- Classification accuracy is the **Proportion** of correct predictions.This is known as EVALUATION MATRIC.\n",
    "- Common **evaluation metric** for classification problems\n",
    "- there are many **evaluation metric** model available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# import metrics module from sklearn\n",
    "# compute classification accuracy for the logistic regression model\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Known as **training accuracy** because you train and test the model on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "print(metrics.accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "print(metrics.accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN (K=1) performed better than KNN (K=5) and gave 100% prediction accuracy. So KNN (K=1) is the best model to use with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN works by looking K observation in the training data with the nearest feature value and makes prediction. It tallies the actual response values of those nearest observation and whichever response value is most popular, is used as predicted response value for the unknown observations. \n",
    "  -- This is the reason, why KNN (K=1) would always has 100% training accuracy.\n",
    "    -- To make prediction for any observation in the training set, **KNN** would search one nearest observation in the training set and would find that exact same observation.\n",
    "    \n",
    "    ### So KNN (K=1) is not the best model to choose to test training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with training and testing on the same data\n",
    "\n",
    "- Goal is to estimate likely performance of a model on **out-of-sample data**\n",
    "- But, maximizing training accuracy rewards **overly complex models** that won't necessarily generalize to future cases.\n",
    "  - Model with high training accuracy may not actually do well when making prediction on **out-of-sample data**.\n",
    "  - Creating Unnecessarily complex models are known as overfitting.\n",
    "    - this complex model learns the noise in the data rather than the signal.\n",
    "         - In the case of KNN with very low value of k(e.g. 1) creates very high complexity model bcoz it follows the noise in the data.\n",
    "- Unnecessarily complex models **overfit** the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overfitting](https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point in above diagram represents an observation, x-y location represents its feature values and color represents its response class.\n",
    "\n",
    "Black line is decision boundry. It is a good boundry to classify future observations as compared to blue spot.\n",
    "\n",
    "Blue line as decision boundry, is overfitting the data. It does perfect job of classifying training data set. But it probably doesn't do well as black line on classifying **out-of-sample-data** \n",
    "\n",
    "Green line has learned noise in the data whereas black line learned the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overfit model, like the one pictured above, has learned the **noise** in the data (the green line) rather than the **signal** (the black line). To avoid overfitting, we'll use a different evaluation procedure that splits our existing data into **training and testing sets**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, choosing a model based on training accuracy can lead to overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Testing on same set of data is not an optimal model of evaluation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Image Credit: [Overfitting](http://commons.wikimedia.org/wiki/File:Overfitting.svg#/media/File:Overfitting.svg) by Chabacano. Licensed under GFDL via Wikimedia Commons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation procedure #2: Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the dataset into two pieces: a **training set** and a **testing set**.\n",
    "2. Train the model on the **training set**.\n",
    "3. Test the model on the **testing set**, and evaluate how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of X and y\n",
    "# X is feature matrix of size 150(row)X4(column)\n",
    "# y is response vector containing 150 response value\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STEP 1: split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_size` paramenter in **tran_test_split()** tells about the ration between proportion of observation assigned to testing set. Here, it is 40% of observation assigned to testing set and rest 60% was assigned to training set.\n",
    "\n",
    "Generally 20-40% of data is used for testing purpose.\n",
    "\n",
    "- How observation is assigned to X_train,y_train, X_test and y_test.\n",
    "    - _Observation is assigned randomly_.\n",
    "        - Running this observation five times on same set of data, it will split the data in 5 different ways.\n",
    "        - To avoid this random assignment, use optional parameter **random_state** and give it integer value. It will split the same dataset in exact same way everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train/test split](http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2015/05/05_train_test_split-e1431626237154.png)\n",
    "\n",
    "Five observation with two feature values and response value.\n",
    "- response value is numeric in diagram, it means that this is a regression problem.\n",
    "X matrix is 5rowX2column.\n",
    "y vector is 5 values.\n",
    "\n",
    "X is divided into X_test(2X2) and X_train(3X2) and y in to y_train(3X1) and y_test(2X1).\n",
    "\n",
    "- Training Data:\n",
    "   - Feature Matrix : X_train\n",
    "   - Response Matrix: y_train\n",
    "   \n",
    "We will make prediction on X_test and compare these prediction with response value in y_test. This comparison will give **testing-accuracy** of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that allows us to calculate the testing accuracy, which better estimates the likely performance of our model on future data. As well, we can locate the optimal tuning parameters for our model by examining its testing accuracy at different levels of model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did this accomplish?\n",
    "\n",
    "- Model can be trained and tested on **different data**\n",
    "- Response values are known for the testing set, and thus **predictions can be evaluated**\n",
    "- **Testing accuracy** is a better estimate than training accuracy of out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(60, 4)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of the new X objects\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of the new y objects\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2: train the model on the training set\n",
    "logreg = LogisticRegression()  # instantiting the LogisticRegression()\n",
    "logreg.fit(X_train, y_train) # fitting it with X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: make predictions for the observation on the testing set by passing X_test to predict() and store it in y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# compare actual response values (y_test) with predicted response values (y_pred)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for KNN with K=5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for KNN with K=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we locate an even better value for K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try K=1 through K=25 and record testing accuracy\n",
    "k_range = list(range(1, 26))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fab122b0e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERERERERERERERERERERERkVr2/wE4YwCY7tdqTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab1230c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import Matplotlib (scientific plotting library)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allow plots to appear within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the relationship between K and testing accuracy\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training accuracy** rises as model complexity increases\n",
    "- **Testing accuracy** penalizes models that are too complex or as well as model that are not complex enough\n",
    "- For KNN models, complexity is determined by the **value of K** (lower value = more complex)\n",
    "\n",
    "- Above behaviour is good for small datasets, but this behaviour(testing accuracy) may not remain for large datasets.\n",
    "\n",
    " Plotting **Testing Accuracy** ***vs*** __model-complexity__ is a very useful way to tune any parameters that relates to __model-complexity__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on out-of-sample data\n",
    "\n",
    "\n",
    "Once you have chosen you model and its optimal parameter (e.g. best value of K for KNN), **retrain** your model on all the available *training-data* before making any new prediction, otherwise you will be throwing away your valuable training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model with the best known parameters \n",
    "# 11 is chosen for K in KNN bcoz it is in the middle of K range with highest testing accuracy. It is the best\n",
    "# parameter.\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "\n",
    "# train the model with X and y (not X_train and y_train)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# make a prediction for an out-of-sample observation\n",
    "knn.predict([[3, 5, 4, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsides of train/test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Provides a **high-variance estimate** of out-of-sample accuracy\n",
    "    - It means that it can change a lot depending upon which observation happen to be in training set vs testing set.\n",
    "    - This limitation can be overcome by using **K-fold cross-validation** by repeating the train/test split process multiple times in a systematic way and averaging the results\n",
    "- **K-fold cross-validation** overcomes this limitation\n",
    "- But, train/test split is still useful because of its **flexibility and speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this week's material at a deeper level, I strongly recommend that you review the two resources below on the bias-variance tradeoff. It's a critical topic that shows up throughout machine learning, and will help you to gain an intuitive sense for why models behave the way they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- Quora: [What is an intuitive explanation of overfitting?](http://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)\n",
    "- Video: [Estimating prediction error](https://www.youtube.com/watch?v=_2ij6eaaSl0&t=2m34s) (12 minutes, starting at 2:34) by Hastie and Tibshirani\n",
    "- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "    - [Guiding questions](https://github.com/justmarkham/DAT8/blob/master/homework/09_bias_variance.md) when reading this article\n",
    "- Video: [Visualizing bias and variance](http://work.caltech.edu/library/081.html) (15 minutes) by Abu-Mostafa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
